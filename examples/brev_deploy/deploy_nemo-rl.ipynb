{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "747ca59c",
   "metadata": {},
   "source": [
    "## 1. NGC Docker Login\n",
    "\n",
    "Authenticate with NVIDIA NGC (GPU Cloud) container registry using your API key.\n",
    "\n",
    "This step is required to pull private NVIDIA containers. Make sure the `NGC_API_KEY` environment variable is set before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed7057-c14c-4cde-81b3-be8f9794791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a880c266",
   "metadata": {},
   "source": [
    "## 2. Configure Cache & Storage Paths\n",
    "\n",
    "Set up ephemeral storage locations for various caches to optimize performance and disk usage:\n",
    "\n",
    "1. **NIM Cache**: Store NVIDIA Inference Microservices cache\n",
    "2. **Docker Storage**: Relocate Docker data root to ephemeral storage\n",
    "3. **Pip Cache**: Cache Python packages for faster installs\n",
    "4. **HuggingFace Cache**: Store downloaded models and datasets\n",
    "5. **Temp Directory**: Set custom temporary file location\n",
    "\n",
    "‚ö†Ô∏è **Note**: This cell requires sudo permissions to modify Docker configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a387f2-0ee0-4fff-99fa-1837f7a71ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, subprocess, time\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Setup NeMo/NIM cache\n",
    "# -------------------------------\n",
    "os.environ[\"LOCAL_NIM_CACHE\"] = \"/ephemeral/cache/nim\"\n",
    "os.makedirs(os.environ[\"LOCAL_NIM_CACHE\"], exist_ok=True)\n",
    "print(f\"LOCAL_NIM_CACHE set to {os.environ['LOCAL_NIM_CACHE']}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Setup Docker ephemeral storage\n",
    "# -------------------------------\n",
    "storage_path = \"/ephemeral/cache/docker\"\n",
    "os.makedirs(storage_path, exist_ok=True)\n",
    "\n",
    "daemon_file = \"/etc/docker/daemon.json\"\n",
    "config = {}\n",
    "try:\n",
    "    config = json.load(open(daemon_file)) if os.path.exists(daemon_file) else {}\n",
    "except PermissionError:\n",
    "    print(\"Cannot read daemon.json. Run with sudo or check path.\")\n",
    "\n",
    "# Update Docker root\n",
    "config[\"data-root\"] = storage_path\n",
    "config_str = json.dumps(config, indent=4)\n",
    "\n",
    "# Write daemon.json (requires sudo)\n",
    "subprocess.run(f\"echo '{config_str}' | sudo tee {daemon_file} > /dev/null\", shell=True, check=True)\n",
    "\n",
    "# Restart Docker\n",
    "subprocess.run(\"sudo systemctl restart docker\", shell=True, check=True)\n",
    "time.sleep(5)\n",
    "\n",
    "# Verify new Docker root\n",
    "docker_root = subprocess.run(\n",
    "    \"docker info | grep 'Docker Root Dir'\",\n",
    "    shell=True, capture_output=True, text=True\n",
    ").stdout.strip()\n",
    "print(\"Docker Root Dir:\", docker_root)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Setup pip cache\n",
    "# -------------------------------\n",
    "pip_cache = \"/ephemeral/cache/pip\"\n",
    "os.makedirs(pip_cache, exist_ok=True)\n",
    "os.environ[\"PIP_CACHE_DIR\"] = pip_cache\n",
    "print(f\"PIP_CACHE_DIR set to {pip_cache}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Setup HuggingFace cache\n",
    "# -------------------------------\n",
    "hf_cache = \"/ephemeral/cache/huggingface\"\n",
    "os.makedirs(hf_cache, exist_ok=True)\n",
    "os.environ[\"HF_HOME\"] = hf_cache\n",
    "print(f\"HF_HOME set to {hf_cache}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Setup tmpdir\n",
    "# -------------------------------\n",
    "tmp_dir = \"/ephemeral/tmp\"\n",
    "os.makedirs(tmp_dir, exist_ok=True)\n",
    "os.environ[\"TMPDIR\"] = tmp_dir\n",
    "print(f\"TMPDIR set to {tmp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7048813e",
   "metadata": {},
   "source": [
    "## 3. Launch NeMo RL Container\n",
    "\n",
    "Start the NeMo RL Docker container in detached mode with:\n",
    "- GPU support enabled\n",
    "- Port 9000 exposed for services\n",
    "- Current directory mounted to `/workspace`\n",
    "- NVIDIA NeMo RL v0.4.0 image\n",
    "\n",
    "The container will run in the background, allowing us to execute commands inside it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5320ff51-186f-47d9-914d-f5b41e7986b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --gpus all --name nemo-rl -it \\\n",
    "  -p 9000:9000 \\\n",
    "  -v \"$(pwd)\":/workspace \\\n",
    "  -w /workspace \\\n",
    "  -d nvcr.io/nvidia/nemo-rl:v0.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db97bd22",
   "metadata": {},
   "source": [
    "## 4. Setup NeMo RL Repository\n",
    "\n",
    "Clone the NeMo RL repository and configure authentication:\n",
    "\n",
    "1. **Clone Repository**: Get the latest NeMo RL code with all submodules\n",
    "2. **Activate Virtual Environment**: Use the pre-configured NeMo RL Python environment\n",
    "3. **HuggingFace Login**: Authenticate to download gated models (like Llama)\n",
    "4. **Weights & Biases**: Set API key for experiment tracking\n",
    "\n",
    "üîë **Security Note**: Replace placeholder tokens with your actual credentials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd367bcb",
   "metadata": {},
   "source": [
    "### Container Started\n",
    "\n",
    "The NeMo RL container is now running. The next cell will set up the repository and authentication inside the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d298c159-540b-4603-b3d0-ec5112300f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"nemo-rl\"\n",
    "\n",
    "!docker exec {container} bash -c \"git clone https://github.com/NVIDIA-NeMo/RL.git nemo-rl --recursive\"\n",
    "!docker exec {container} bash -c \"cd nemo-rl && git submodule update --init --recursive\"\n",
    "\n",
    "# Activate NeMo RL venv\n",
    "!docker exec {container} bash -c \"source /opt/nemo_rl_venv/bin/activate\"\n",
    "\n",
    "# HuggingFace login\n",
    "!docker exec {container} bash -c \"huggingface-cli login --token hf_********\"\n",
    "\n",
    "# WANDB API key\n",
    "!docker exec {container} bash -c 'export WANDB_API_KEY=\"*****\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a37b13",
   "metadata": {},
   "source": [
    "## 5. Run DPO Training\n",
    "\n",
    "Execute Direct Preference Optimization (DPO) training using the NeMo RL framework.\n",
    "\n",
    "### Training Configuration:\n",
    "- **Model**: `meta-llama/Llama-3.2-1B-Instruct` (1B parameter instruction-tuned model)\n",
    "- **GPUs**: 1 GPU per node\n",
    "- **Steps**: 10 training steps (quick demo - increase for production)\n",
    "- **Output**: Checkpoints saved to `./results/dpo/step_10/`\n",
    "\n",
    "### What is DPO?\n",
    "DPO is a reinforcement learning technique that trains models to align with human preferences by learning from preference pairs, without requiring a separate reward model.\n",
    "\n",
    "‚è±Ô∏è **Expected Time**: Several minutes depending on GPU and model size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdccb300",
   "metadata": {},
   "source": [
    "### Repository Configured\n",
    "\n",
    "Authentication and repository setup complete. Ready to run DPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca800a68-7d8c-4a4d-a42f-41c71a5154ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "container = \"nemo-rl\"\n",
    "\n",
    "!docker exec -it $container bash -c 'source /opt/nemo_rl_venv/bin/activate && \\\n",
    "uv run python nemo-rl/examples/run_dpo.py \\\n",
    "cluster.gpus_per_node=1 \\\n",
    "dpo.max_num_steps=10 \\\n",
    "policy.model_name=meta-llama/Llama-3.2-1B-Instruct \\\n",
    "policy.tokenizer.name=meta-llama/Llama-3.2-1B-Instruct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c1b3fa",
   "metadata": {},
   "source": [
    "## 6. Convert Model to HuggingFace Format\n",
    "\n",
    "Convert the trained DCP (Distributed Checkpoint) format to HuggingFace format for wider compatibility.\n",
    "\n",
    "### Conversion Details:\n",
    "- **Input**: DCP checkpoint from `./results/dpo/step_10/policy/weights`\n",
    "- **Output**: HuggingFace-compatible model at `./results/dpo/step_10/hf`\n",
    "- **Benefits**: Enables use with HuggingFace Transformers library and ecosystem\n",
    "\n",
    "This conversion makes the model easier to share, deploy, and integrate with standard tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff1597",
   "metadata": {},
   "source": [
    "### Training Complete\n",
    "\n",
    "DPO training finished. The trained model checkpoints are saved in `./results/dpo/step_10/`. The next step converts these to HuggingFace format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb74402-c499-4f3f-b720-a84212b4bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"nemo-rl\"\n",
    "\n",
    "!docker exec {container} bash -c \"source /opt/nemo_rl_venv/bin/activate && \\\n",
    "    uv run nemo-rl/examples/converters/convert_dcp_to_hf.py \\\n",
    "    --config ./results/dpo/step_10/config.yaml \\\n",
    "    --dcp-ckpt-path ./results/dpo/step_10/policy/weights \\\n",
    "    --hf-ckpt-path ./results/dpo/step_10/hf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35be3829",
   "metadata": {},
   "source": [
    "### Conversion Complete\n",
    "\n",
    "Model successfully converted to HuggingFace format. Now let's test the model with a local inference script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b910ca",
   "metadata": {},
   "source": [
    "## 7. Local Inference Testing\n",
    "\n",
    "Test the converted model with a simple inference script to verify it works correctly.\n",
    "\n",
    "### Inference Script:\n",
    "- Loads the HuggingFace model and tokenizer\n",
    "- Uses `bfloat16` precision for efficiency\n",
    "- Generates up to 50 new tokens\n",
    "- Tests with a science question about photosynthesis\n",
    "\n",
    "This step validates that the model conversion was successful before deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54646820",
   "metadata": {},
   "source": [
    "### Inference Script Created\n",
    "\n",
    "The `inference.py` script has been written. Execute it in the next cell to verify the model works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792f4852-5d99-4ad7-9c4d-98079ad899df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference.py\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "hf_path = \"./results/dpo/step_10/hf/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_path, torch_dtype=torch.bfloat16)\n",
    "model.eval()\n",
    "\n",
    "prompt = \"How does photosynthesis work in plants?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "out = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b206b",
   "metadata": {},
   "source": [
    "### Local Inference Test Complete\n",
    "\n",
    "The model generated output successfully. Now let's convert to SafeTensors format for better security and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc920a-1e0f-4930-8ddd-32e6ceff6e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"nemo-rl\"\n",
    "!docker exec {container} bash -c \"source /opt/nemo_rl_venv/bin/activate && python inference.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d654332a-d760-4924-860c-3f6dd25a726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile convert.py\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "src = \"./results/dpo/step_10/hf\"\n",
    "dst = \"./results/dpo/step_10/hf_st\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(src)\n",
    "model.save_pretrained(dst, safe_serialization=True)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(src)\n",
    "tok.save_pretrained(dst)\n",
    "\n",
    "print(\"Saved to:\", dst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aaca91",
   "metadata": {},
   "source": [
    "## 8. Convert to SafeTensors Format\n",
    "\n",
    "Convert the HuggingFace model to use SafeTensors serialization for improved security and performance.\n",
    "\n",
    "### SafeTensors Benefits:\n",
    "- **Security**: Prevents arbitrary code execution vulnerabilities\n",
    "- **Performance**: Faster loading times with zero-copy deserialization\n",
    "- **Reliability**: Better error handling and validation\n",
    "- **Compatibility**: Works with all major ML frameworks\n",
    "\n",
    "The converted model is saved to `./results/dpo/step_10/hf_st` (st = safe tensors).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227d8c20",
   "metadata": {},
   "source": [
    "### SafeTensors Script Created\n",
    "\n",
    "The `convert.py` script has been written. Execute it in the next cell to convert the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5dc77f-d47b-4b2d-b700-ca4af89c1cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"nemo-rl\"\n",
    "!docker exec {container} bash -c \"source /opt/nemo_rl_venv/bin/activate && python convert.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1c0d0",
   "metadata": {},
   "source": [
    "### SafeTensors Conversion Complete\n",
    "\n",
    "Model successfully converted to SafeTensors format at `./results/dpo/step_10/hf_st`. Ready for NIM deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fceb99",
   "metadata": {},
   "source": [
    "## 9. Deploy with NVIDIA NIM\n",
    "\n",
    "Deploy the trained model using NVIDIA Inference Microservices (NIM) for production-ready serving.\n",
    "\n",
    "### NIM Configuration:\n",
    "- **Container**: `MultiLLM-NIM` running the latest NVIDIA LLM-NIM image\n",
    "- **Model Name**: Exposed as `dpo-llm` via the API\n",
    "- **GPU Support**: All available GPUs with 16GB shared memory\n",
    "- **Port**: Service available on `localhost:8000`\n",
    "- **Caching**: Uses ephemeral storage for optimal performance\n",
    "\n",
    "### What is NIM?\n",
    "NVIDIA NIM provides optimized inference microservices with:\n",
    "- Low latency and high throughput\n",
    "- OpenAI-compatible API endpoints\n",
    "- Built-in performance optimizations\n",
    "- Easy deployment and scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be452f",
   "metadata": {},
   "source": [
    "### NIM Configuration Set\n",
    "\n",
    "Container and model settings configured. The next cell will launch the NIM container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49709a84-1f6a-40fc-ad6a-3ca570fe0b76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "#   MultiLLM-NIM Container Launcher\n",
    "#   (Detached mode)\n",
    "# ===============================\n",
    "\n",
    "# Choose container name\n",
    "CONTAINER_NAME = \"MultiLLM-NIM\"\n",
    "\n",
    "# NGC Multi-LLM NIM repo\n",
    "Repository = \"nim/nvidia/llm-nim\"\n",
    "TAG = \"latest\"\n",
    "IMG_NAME = f\"nvcr.io/{Repository}:{TAG}\"\n",
    "\n",
    "# Path to your local HF DPO model\n",
    "LOCAL_MODEL_DIR = \"./results/dpo/step_10/hf_st\"\n",
    "\n",
    "# Name to expose the served model\n",
    "NIM_SERVED_MODEL_NAME = \"dpo-llm\"\n",
    "\n",
    "# Local NIM cache (you chose ephemeral)\n",
    "LOCAL_NIM_CACHE = \"/ephemeral/cache/nim\"\n",
    "\n",
    "# Create cache directory\n",
    "!mkdir -p \"{LOCAL_NIM_CACHE}\"\n",
    "!chmod -R a+w \"{LOCAL_NIM_CACHE}\"\n",
    "\n",
    "print(\"Starting MultiLLM-NIM container in detached mode...\")\n",
    "print(\"Container:\", CONTAINER_NAME)\n",
    "print(\"Image:\", IMG_NAME)\n",
    "print(\"Model Path:\", LOCAL_MODEL_DIR)\n",
    "print(\"NIM Cache:\", LOCAL_NIM_CACHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5fbe9c",
   "metadata": {},
   "source": [
    "### NIM Container Launched\n",
    "\n",
    "The MultiLLM-NIM container is starting up. Use the health check to wait for it to be ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4451aa29-82a8-4f19-9873-aaedd122cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Run the container DETACHED\n",
    "# -------------------------------\n",
    "!docker run -d --rm --name={CONTAINER_NAME} \\\n",
    "  --runtime=nvidia \\\n",
    "  --gpus all \\\n",
    "  --shm-size=16GB \\\n",
    "  -e NIM_MODEL_PROFILE=\"e2f00b2cbfb168f907c8d6d4d40406f7261111fbab8b3417a485dcd19d10cc98\" \\\n",
    "  -e NIM_MODEL_NAME=\"/opt/models/local_model\" \\\n",
    "  -e NIM_SERVED_MODEL_NAME={NIM_SERVED_MODEL_NAME} \\\n",
    "  -v \"{LOCAL_MODEL_DIR}:/opt/models/local_model\" \\\n",
    "  -v \"{LOCAL_NIM_CACHE}:/opt/nim/.cache\" \\\n",
    "  -u $(id -u) \\\n",
    "  -p 8000:8000 \\\n",
    "  {IMG_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d3c397",
   "metadata": {},
   "source": [
    "## 10. Health Check\n",
    "\n",
    "Wait for the NIM service to be fully ready before sending inference requests.\n",
    "\n",
    "This cell continuously polls the `/v1/health/ready` endpoint until the service reports ready status. The container needs time to:\n",
    "- Load the model into GPU memory\n",
    "- Initialize the inference engine\n",
    "- Start the API server\n",
    "\n",
    "‚è±Ô∏è **Expected Wait Time**: 1-5 minutes depending on model size and hardware.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b88a04",
   "metadata": {},
   "source": [
    "### NIM Service Ready\n",
    "\n",
    "The service is ready to accept inference requests. Test it with the completions API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb0b9c-39ff-4709-889f-e2e891f79b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://localhost:8000/v1/health/ready' #make sure the LLM NIM port is correct\n",
    "headers = {'accept': 'application/json'}\n",
    "\n",
    "print(\"Checking MultiLLM NIM readiness...\")\n",
    "while True:\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if data.get(\"message\") == \"Service is ready.\":\n",
    "                print(\"LLM NIM is ready.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"LLM NIM is not ready. Waiting for 30 seconds...\")\n",
    "        else:\n",
    "            print(f\"Unexpected status code {response.status_code}. Waiting for 30 seconds...\")\n",
    "    except requests.ConnectionError:\n",
    "        print(\"LLM NIM is not ready. Waiting for 30 seconds...\")\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcfca33",
   "metadata": {},
   "source": [
    "### API Test Complete\n",
    "\n",
    "The model responded to the completion request successfully. You can now stop the container or continue using the service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c762c93",
   "metadata": {},
   "source": [
    "## 11. Test Completions API\n",
    "\n",
    "Test the deployed model using the OpenAI-compatible completions endpoint.\n",
    "\n",
    "### Request Parameters:\n",
    "- **model**: `dpo-llm` (our trained DPO model)\n",
    "- **prompt**: A starter phrase to complete\n",
    "- **max_tokens**: Maximum number of tokens to generate (64)\n",
    "\n",
    "The API will return a JSON response with the model's completion of the prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1d8d9-4056-4c1c-afbc-0ec3ee2485e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST 'http://localhost:8000/v1/completions' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\"model\": \"dpo-llm\", \"prompt\": \"The sky appears blue because\", \"max_tokens\": 64}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1ec8c1",
   "metadata": {},
   "source": [
    "## 12. Cleanup\n",
    "\n",
    "Stop the MultiLLM-NIM container to free up GPU resources.\n",
    "\n",
    "üßπ **Note**: The `--rm` flag used when starting the container ensures it's automatically removed after stopping, keeping your Docker environment clean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33416929-22c2-44e1-9b1c-6e418364ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop MultiLLM-NIM nemo-rl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
